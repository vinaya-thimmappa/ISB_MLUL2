{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning - Unsupervised Learning - 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Temporal Anomaly Detection\n",
    "We will do detection for temporal data. Recall that the challenge is the temporal data is a combination of seasonal patterns and/or cycles, an actual overall trend, and noise. So in order to do detection we must first take these into consideration and make our data stationary. This will allow us to we have an expectation for what we should see at even given time point, and a deviation (residual) capturing the distance between our expectation and observation.\n",
    "\n",
    "### Seasonal and Trend decomposition using Loess\n",
    "Seasonal and Trend decomposition using Loess (STL) is a very versatile and robust method for decomposing time series developed by [(Cleveland et al., 1900)](http://cs.wellesley.edu/~cs315/Papers/stl%20statistical%20model.pdf). In class we discussed [weighted and moving averages](https://en.wikipedia.org/wiki/Moving_average), but Loess is another technique for estimating nonlinear relationships via decomposition. Specifically, STL decomposes time series data into seasonal, trend, and irregular components using loess and plots the components separately. We will see this in action by exploring the `elecequip` which shows the number of new orders for electrical equipment (computer, electronic and optical products) in the Euro area (16 countries). The data have been adjusted by working days and normalized so a value of 100 corresponds to 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df = pd.read_csv(\"./data/elecequip.csv\")\n",
    "elec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: \n",
    "\n",
    "1. We convert to date time format\n",
    "2. Apply Month-Year as an index. \n",
    "3. Finally set the appropriate frequency. The frequency can be infered from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df['Month-Year'] = pd.to_datetime(elec_df['Month-Year'], format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df.set_index(['Month-Year'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df.index.freq = pd.infer_freq(elec_df.index)\n",
    "elec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df['Value'].plot()\n",
    "plt.title(\"Electrical equipment manufacturing\")\n",
    "plt.ylabel(\"New orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we fit the STL model to this time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "stl = STL(elec_df['Value'], trend=13, seasonal = 5)\n",
    "res = stl.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can plot the trend line along with actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots()\n",
    "res.trend.plot(ax=ax)\n",
    "elec_df['Value'].plot(ax=ax)\n",
    "plt.title(\"Electrical equipment manufacturing\")\n",
    "plt.ylabel(\"New orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the trend component in black and the original data in ornage. The trend shows the overall movement in the series, ignoring the seasonality and noise components.\n",
    "\n",
    "We can also show an additive decomposition of these data using STL by plotting the `fit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can visualize the trend, seasonality and residuals identified by STL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = res.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where have the data decomposed into data overall trend, seasonal variation, and then left over noise. **The left over noise, known as residuals, can be evaluated for outliers**. We should look at the density noise component and see if it (mostly) look gaussian or not,if so then we can, check it for outliers using General Extreme Studentized Deviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = res.resid\n",
    "sns.kdeplot(noise)\n",
    "plt.title(\"Density of the Noice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Extreme Studentized Deviate\n",
    "In our lecture we discussed [GESD](http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm), which is a way to identify outliers in our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GESD Helper functions\n",
    "\n",
    "Below is the helper function. **Read through it to see how it is working.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.stats as stats\n",
    "def gesd(obs, alpha, value_zscore = np.nan, r = np.nan):\n",
    "    n = len(obs)\n",
    "    if (pd.isna(r)): # by default, set upper bound on number of outliers 'r' to 1/2 sample size\n",
    "        r = math.floor(n/2)\n",
    "    R = np.zeros(shape = r) # test statistics for 'r' outliers\n",
    "    lamb = np.zeros(shape = r) # critical values for 'r' outliers\n",
    "    outlier_ind = np.zeros(shape = r) # removed outlier observation values\n",
    "    outlier_val = np.zeros(shape = r) # removed outlier observation values\n",
    "    \n",
    "    m = 0   # number of outliers\n",
    "    obs_new = np.array(obs) # temporary observation values\n",
    "    \n",
    "    ### Find outliers ####\n",
    "    for i in range(r):\n",
    "        \n",
    "        #### Compute test statistic ####\n",
    "        if (str.lower(value_zscore) == \"yes\"):\n",
    "            z = abs(obs_new) # If Z-score is alrealy computed\n",
    "        elif (str.lower(value_zscore) == \"no\"):\n",
    "            z = abs(obs_new - np.mean(obs_new))/np.std(obs_new) # Z-scores\n",
    "        else:\n",
    "            print(\"ERROR! Inappropriate value for value.score=[YES|NO]\")\n",
    "        \n",
    "        max_ind = np.argmax(z) # in case of ties, return first one\n",
    "        R[i] = z[max_ind] # max Z-score\n",
    "        \n",
    "        outlier_val[i] = obs_new[max_ind] # removed outlier observation values\n",
    "        outlier_ind[i] = max_ind # index of removed outlier observation values\n",
    "        \n",
    "        obs_new = np.delete(obs_new, max_ind) # remove observation that maximizes |x_i - x_mean|\n",
    "        \n",
    "        #### Compute critical values ####\n",
    "        p = 1 - alpha/(2*(n-i+1)) # probability\n",
    "        t_pv = stats.t.ppf(p, (n-i-1)) # Critical value from Student's t distribution\n",
    "        lamb[i] = ((n-i)*t_pv) / (math.sqrt((n-i-1+t_pv**2)*(n-i+1)))\n",
    "        \n",
    "        #### Find the exact number of outliers: largest 'i' such that R_i > lambda_i ####\n",
    "        #print(i, R[i], lamb[i])\n",
    "        if ( (not pd.isna(R[i])) & (not pd.isna(lamb[i])) ): # stats.t.ppf can produces nans\n",
    "            if (R[i] > lamb[i]):\n",
    "                m = i + 1\n",
    "    vals = pd.DataFrame.from_dict(dict(zip(['NumOutliers','TestStatistic','CriticalValue'], \n",
    "                                           [range(r),R,lamb])))\n",
    "    \n",
    "    #print (m)\n",
    "    outlier_rank = np.zeros(shape = n) - 1\n",
    "    if ( m> -1):\n",
    "        for i in range(m):\n",
    "            outlier_rank[np.where(obs==outlier_val[i])] = i\n",
    "            #print(outlier_rank[np.where(obs==outlier_val[i])])\n",
    "    \n",
    "    res = dict()\n",
    "    res['table'] = vals\n",
    "    res['num_outliers'] = sum(outlier_rank!= -1)\n",
    "    res['outlier_rank'] = outlier_rank\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assuming no more than 5% of my data will be an outliers\n",
    "r = round(len(noise) * 0.05)\n",
    "\n",
    "#the critical value of my test\n",
    "alpha = 0.1\n",
    "gesd_result = gesd(noise, alpha,\"No\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by `gesd` will tell you how many outliers it detected in its `num_outliers` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesd_result['num_outliers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, in its `table` property, the `gesd` object will produce a table for each row $i$, which compares $R_i$ (the test statistics) to $\\lambda_i$ (the critical value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesd_result['table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in its `outlier_rank` property, the `gesd` object will produce a vector (in order of the original data points) which contains the rank of the outlier (or -1 if the point is not an outlier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesd_result['outlier_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = gesd_result['outlier_rank']>-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the detected object\n",
    "\n",
    "We can use the following helper function to plot the original time series and highlight the outliers detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "def plot_detected(df, detected, column_name):\n",
    "    \n",
    "    x = df.index\n",
    "    y = df[column_name].values\n",
    "    fig, ax = plt.subplots(figsize = (10,6))\n",
    "    ax.plot_date(x, y, linestyle = 'solid')\n",
    "    \n",
    "    detected_ind = np.where(detected)[0]\n",
    "    \n",
    "    for i in range(len(detected_ind)):\n",
    "        print(detected_ind[i])\n",
    "        ax.annotate('Outlier', (mdates.date2num(x[detected_ind[i]]), y[detected_ind[i]]), xytext=(60, 60), \n",
    "            textcoords='offset points', arrowprops=dict(arrowstyle='-|>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highlighting the original points whose noise is extreme\n",
    "plot_detected(elec_df, detected, column_name = 'Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
